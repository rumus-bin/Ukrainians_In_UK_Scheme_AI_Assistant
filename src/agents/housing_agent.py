"""Housing and life support specialized agent."""

from typing import Optional, Dict
from src.agents.base_agent import BaseAgent, AgentResponse
from src.agents.mcp_client import get_mcp_client, WebSearchResult
from src.utils.config import get_settings
from src.utils.logger import get_logger

logger = get_logger()


class HousingAgent(BaseAgent):
    """Specialized agent for housing and life support questions."""

    def __init__(self):
        """Initialize Housing Agent."""
        settings = get_settings()
        super().__init__(
            name="housing_agent",
            model=settings.housing_agent_model,
            topic_filter=None  # Disable topic filter - use semantic search only
        )

        # Initialize MCP client for web access
        self.mcp_client = get_mcp_client()
        self.use_web_search = False  # Disabled - use RAG database only

    def _build_system_prompt(self) -> str:
        """Build housing agent system prompt with safety rules."""
        return """–¢–∏ - —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –ø–æ–º—ñ—á–Ω–∏–∫ –∑ –ø–∏—Ç–∞–Ω—å –∂–∏—Ç–ª–∞ —Ç–∞ –∂–∏—Ç—Ç—è —É –í–µ–ª–∏–∫—ñ–π –ë—Ä–∏—Ç–∞–Ω—ñ—ó –¥–ª—è —É–∫—Ä–∞—ó–Ω—Ü—ñ–≤.

‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–õ–ò–í–Ü –ü–†–ê–í–ò–õ–ê:
1. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò –ø—Ä–æ –í–µ–ª–∏–∫—É –ë—Ä–∏—Ç–∞–Ω—ñ—é (UK). –ù–Ü–ö–û–õ–ò –ø—Ä–æ –£–∫—Ä–∞—ó–Ω—É –∞–±–æ —ñ–Ω—à—ñ –∫—Ä–∞—ó–Ω–∏
2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ –Ω–∞–¥–∞–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (gov.uk, opora.uk)
3. –Ø–∫—â–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ü–û–†–û–ñ–ù–Ü–ô –∞–±–æ –ù–ï –º—ñ—Å—Ç–∏—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è:
   - –ó–ê–ë–û–†–û–ù–ï–ù–û –≤–∏–≥–∞–¥—É–≤–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é
   - –ó–ê–ë–û–†–û–ù–ï–ù–û –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∑–∞–≥–∞–ª—å–Ω—ñ –∑–Ω–∞–Ω–Ω—è –ø—Ä–æ —ñ–Ω—à—ñ –∫—Ä–∞—ó–Ω–∏
   - –û–ë–û–í'–Ø–ó–ö–û–í–û —Å–∫–∞–∂–∏: "–ù–∞ –∂–∞–ª—å, —É –º–æ—ó–π –±–∞–∑—ñ –∑–Ω–∞–Ω—å –Ω–µ–º–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ [—Ç–µ–º–∞] —É –í–µ–ª–∏–∫—ñ–π –ë—Ä–∏—Ç–∞–Ω—ñ—ó. –†–µ–∫–æ–º–µ–Ω–¥—É—é –∑–≤–µ—Ä–Ω—É—Ç–∏—Å—è –¥–æ gov.uk –∞–±–æ opora.uk"
4. –í–°–Ü –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –¢–Ü–õ–¨–ö–ò –ø—Ä–æ UK, –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –Ω–µ –≤–∫–∞–∑–∞–≤ –∫—Ä–∞—ó–Ω—É
5. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é
6. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –µ–º–æ–¥–∑—ñ –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ
7. –ó–∞–≤–∂–¥–∏ –¥–æ–¥–∞–≤–∞–π disclaimer –≤ –∫—ñ–Ω—Ü—ñ

–¢–í–û–Ø –°–ü–ï–¶–Ü–ê–õ–Ü–ó–ê–¶–Ü–Ø (–¢–Ü–õ–¨–ö–ò –¥–ª—è –í–µ–ª–∏–∫–æ—ó –ë—Ä–∏—Ç–∞–Ω—ñ—ó):
- Social housing —Ç–∞ council housing —É UK
- –ñ–∏—Ç–ª–æ —Ç–∞ –ø—Ä–∞–≤–∞ –æ—Ä–µ–Ω–¥–∞—Ä—ñ–≤ —É UK
- NHS —Ç–∞ GP —Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—è —É UK
- –ú–µ–¥–∏—á–Ω—ñ –ø–æ—Å–ª—É–≥–∏ —É UK
- –®–∫–æ–ª–∏ —Ç–∞ –æ—Å–≤—ñ—Ç–∞ —É UK
- –ú—É–Ω—ñ—Ü–∏–ø–∞–ª—å–Ω—ñ –ø–æ—Å–ª—É–≥–∏ —É UK

–°–¢–†–£–ö–¢–£–†–ê –í–Ü–î–ü–û–í–Ü–î–Ü:
üè† [–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è]

üìù –ö—Ä–æ–∫–∏ (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ):
1. [–ü–µ—Ä—à–∏–π –∫—Ä–æ–∫]
2. [–î—Ä—É–≥–∏–π –∫—Ä–æ–∫]
3. [–¢—Ä–µ—Ç—ñ–π –∫—Ä–æ–∫]

üí° –ö–æ—Ä–∏—Å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è:
[–î–æ–¥–∞—Ç–∫–æ–≤—ñ –¥–µ—Ç–∞–ª—ñ]

üîó –ö–æ—Ä–∏—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è:
[–ü–æ—Å–∏–ª–∞–Ω–Ω—è –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É]

‚ö†Ô∏è –¶–µ –∑–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è, –Ω–µ —é—Ä–∏–¥–∏—á–Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—è. –î–ª—è –ø—Ä–∞–≤–æ–≤–∏—Ö –ø–∏—Ç–∞–Ω—å –∑–≤–µ—Ä–Ω—ñ—Ç—å—Å—è –¥–æ —Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç–∞.

–©–û –ù–ï –ú–û–ñ–ù–ê –†–û–ë–ò–¢–ò:
‚ùå –î–∞–≤–∞—Ç–∏ —é—Ä–∏–¥–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ —â–æ–¥–æ –¥–æ–≥–æ–≤–æ—Ä—ñ–≤
‚ùå –†–∞–¥–∏—Ç–∏ –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—É–≤–∞—Ç–∏ –∑ –æ—Ä–µ–Ω–¥–æ–¥–∞–≤—Ü–µ–º –±–µ–∑ –ø—ñ–¥—Å—Ç–∞–≤
‚ùå –û–±—ñ—Ü—è—Ç–∏ —à–≤–∏–¥–∫–µ –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –ø—Ä–æ–±–ª–µ–º –∑ NHS
‚ùå –ì–∞—Ä–∞–Ω—Ç—É–≤–∞—Ç–∏ –∑–∞—Ä–∞—Ö—É–≤–∞–Ω–Ω—è –¥—ñ—Ç–µ–π –¥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó —à–∫–æ–ª–∏

–©–û –ü–û–¢–†–Ü–ë–ù–û –†–û–ë–ò–¢–ò:
‚úÖ –ü–æ—è—Å–Ω—é–≤–∞—Ç–∏ –ø—Ä–æ—Ü–µ–¥—É—Ä–∏ —Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—ó
‚úÖ –î–∞–≤–∞—Ç–∏ –∫–æ–Ω—Ç–∞–∫—Ç–∏ –æ—Ñ—ñ—Ü—ñ–π–Ω–∏—Ö —Å–ª—É–∂–±
‚úÖ –û–ø–∏—Å—É–≤–∞—Ç–∏ –ø—Ä–∞–≤–∞ —Ç–∞ –æ–±–æ–≤'—è–∑–∫–∏
‚úÖ –ù–∞–¥–∞–≤–∞—Ç–∏ –ø–æ–∫—Ä–æ–∫–æ–≤—ñ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó
‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–≤–∞—Ç–∏ –ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó"""

    async def process(
        self,
        query: str,
        context: Optional[Dict] = None
    ) -> AgentResponse:
        """
        Process user query with enhanced web search capability.

        This override adds web search functionality to supplement RAG.

        Args:
            query: User query text
            context: Optional additional context

        Returns:
            AgentResponse with generated text and metadata
        """
        import time
        start_time = time.time()

        try:
            logger.info(f"{self.name}: Processing query with web search: '{query[:50]}...'")

            # Initialize retriever if needed
            if not self.retriever._connected:
                self.retriever.initialize()

            # Step 1: Retrieve context from RAG (primary source)
            retrieval_result = await self._retrieve_context(query)

            # Step 2: Determine if web search is needed
            needs_web_search = self._should_use_web_search(query, retrieval_result)

            web_content = None
            if needs_web_search and self.use_web_search:
                logger.info(f"{self.name}: Supplementing RAG with web search")
                web_content = await self._perform_web_search(query)

            # Step 3: Combine RAG and web search results
            combined_context = self._combine_contexts(
                retrieval_result.context,
                web_content
            )

            # Step 4: Generate response using LLM
            response_text = await self._generate_llm_response(
                query=query,
                context=combined_context,
                system_prompt=self.get_system_prompt()
            )

            processing_time = time.time() - start_time

            # Build sources list (RAG + web)
            all_sources = retrieval_result.sources.copy()
            if web_content:
                all_sources.append({
                    "url": web_content.source_url,
                    "title": web_content.title,
                    "source": "web_search",
                    "fresh": True
                })

            logger.info(
                f"{self.name}: Generated response in {processing_time:.2f}s "
                f"(RAG: {retrieval_result.found_documents} docs, Web: {1 if web_content else 0})"
            )

            return AgentResponse(
                text=response_text,
                sources=all_sources,
                agent_name=self.name,
                confidence=self._calculate_confidence(retrieval_result),
                processing_time=processing_time,
                metadata={
                    "query": query,
                    "found_documents": retrieval_result.found_documents,
                    "used_web_search": web_content is not None,
                    "model": self.model
                }
            )

        except Exception as e:
            logger.error(f"{self.name}: Error processing query: {e}")
            processing_time = time.time() - start_time

            # Return error response
            return AgentResponse(
                text=self._get_error_response(),
                sources=[],
                agent_name=self.name,
                confidence=0.0,
                processing_time=processing_time,
                metadata={"error": str(e)}
            )

    def _should_use_web_search(self, query: str, retrieval_result) -> bool:
        """
        Determine if web search should be used to supplement RAG.

        Args:
            query: User query
            retrieval_result: RAG retrieval result

        Returns:
            True if web search should be used
        """
        # Use web search if:
        # 1. Very few documents found in RAG
        if retrieval_result.found_documents < 2:
            logger.debug("Web search triggered: low RAG document count")
            return True

        # 2. Query mentions "recent", "latest", "new", "current"
        freshness_keywords = ['recent', 'latest', 'new', 'current', 'today', 'now',
                             '–æ—Å—Ç–∞–Ω–Ω—ñ', '–Ω–æ–≤—ñ', '–∞–∫—Ç—É–∞–ª—å–Ω—ñ', '–ø–æ—Ç–æ—á–Ω—ñ', '—Å–≤—ñ–∂—ñ']
        if any(keyword in query.lower() for keyword in freshness_keywords):
            logger.debug("Web search triggered: freshness keywords detected")
            return True

        # 3. Query is about specific government schemes (might have recent updates)
        scheme_keywords = ['homes for ukraine', 'ukraine scheme', '—Å—Ö–µ–º–∞', '–ø—Ä–æ–≥—Ä–∞–º–∞']
        if any(keyword in query.lower() for keyword in scheme_keywords):
            logger.debug("Web search triggered: government scheme query")
            return True

        # Otherwise, RAG should be sufficient
        return False

    async def _perform_web_search(self, query: str) -> Optional[WebSearchResult]:
        """
        Perform web search using direct scraper access.

        Args:
            query: User query

        Returns:
            WebSearchResult or None
        """
        try:
            # Use direct scraper import (simpler than MCP for now)
            import sys
            sys.path.insert(0, '/app/mcp-servers/web-scraper')

            from scrapers.govuk_scraper import GovUkScraper
            from scrapers.opora_scraper import OporaUkScraper

            # Determine which source to search based on query
            query_lower = query.lower()

            # Check for NHS/healthcare keywords
            nhs_keywords = ['nhs', 'doctor', 'gp', 'hospital', 'health', 'medical',
                           '–ª—ñ–∫–∞—Ä', '–ª—ñ–∫–∞—Ä–Ω—è', '–∑–¥–æ—Ä–æ–≤\'—è', '–º–µ–¥–∏—á–Ω']
            is_nhs_query = any(keyword in query_lower for keyword in nhs_keywords)

            # Get base URLs from settings
            from src.utils.config import get_settings
            settings = get_settings()

            # Initialize scrapers with configured base URLs
            govuk = GovUkScraper(
                user_agent='UkraineSupportBot/1.0',
                cache_dir='/app/mcp-servers/web-scraper/cache/html',
                base_url=settings.scraper_gov_uk_base,
                cache_ttl=86400,
                respect_robots=True
            )

            opora = OporaUkScraper(
                user_agent='UkraineSupportBot/1.0',
                cache_dir='/app/mcp-servers/web-scraper/cache/html',
                base_url=settings.scraper_opora_uk_base,
                cache_ttl=86400,
                respect_robots=True
            )

            if is_nhs_query:
                logger.info("Searching NHS information on Gov.uk")
                scraped = govuk.get_nhs_info(use_cache=True)

                if not scraped or len(scraped.content) < 200:
                    logger.info("Fallback: Searching NHS on Opora.uk")
                    opora_scraped = opora.get_nhs_info(use_cache=True)
                    if opora_scraped:
                        scraped = opora_scraped
            else:
                # Housing query
                logger.info("Searching housing information on Gov.uk")
                scraped = govuk.get_housing_info(topic='ukraine', use_cache=True)

                if not scraped or len(scraped.content) < 200:
                    logger.info("Supplementing with Opora.uk housing info")

                    # Check if pagination is enabled and if base_url is a blog
                    use_pagination = (
                        settings.scraper_pagination_enabled and
                        '/blog' in settings.scraper_opora_uk_base.lower()
                    )

                    if use_pagination:
                        logger.info(f"Using pagination to fetch from {settings.scraper_opora_uk_base}")
                        # Use pagination for blog listing
                        opora_scraped = opora.fetch_with_pagination(
                            start_url=settings.scraper_opora_uk_base,
                            max_pages=settings.scraper_max_pages,
                            use_cache=True,
                            timeout_seconds=settings.scraper_pagination_timeout_seconds
                        )
                    else:
                        # Use regular single-page fetch
                        opora_scraped = opora.get_housing_info(use_cache=True)

                    if opora_scraped and len(opora_scraped.content) > len(scraped.content if scraped else ""):
                        scraped = opora_scraped

            if scraped:
                return WebSearchResult(
                    content=scraped.content,
                    source_url=scraped.url,
                    title=scraped.title,
                    metadata=scraped.metadata
                )

            return None

        except Exception as e:
            logger.error(f"Web search failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def _combine_contexts(
        self,
        rag_context: str,
        web_content: Optional[WebSearchResult]
    ) -> str:
        """
        Combine RAG context with web search results.

        Args:
            rag_context: Context from RAG system
            web_content: Web search result

        Returns:
            Combined context string
        """
        if not web_content:
            return rag_context

        # Build combined context
        parts = []

        if rag_context:
            parts.append("=== –Ü–ù–§–û–†–ú–ê–¶–Ü–Ø –ó –ë–ê–ó–ò –ó–ù–ê–ù–¨ ===")
            parts.append(rag_context)
            parts.append("")

        if web_content and web_content.content:
            parts.append("=== –ê–ö–¢–£–ê–õ–¨–ù–ê –Ü–ù–§–û–†–ú–ê–¶–Ü–Ø –ó –í–ï–ë-–î–ñ–ï–†–ï–õ ===")
            parts.append(f"–î–∂–µ—Ä–µ–ª–æ: {web_content.source_url}")
            parts.append(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {web_content.title}")
            parts.append("")
            parts.append(web_content.content)
            parts.append("")

        combined = "\n".join(parts)

        logger.debug(f"Combined context length: {len(combined)} chars "
                    f"(RAG: {len(rag_context)}, Web: {len(web_content.content) if web_content else 0})")

        return combined
