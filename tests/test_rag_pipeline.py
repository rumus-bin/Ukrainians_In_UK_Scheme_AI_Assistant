#!/usr/bin/env python3
"""
End-to-end test for the complete RAG pipeline.

Tests the full workflow:
1. Document ingestion (scrape â†’ chunk â†’ embed â†’ store)
2. Document retrieval (query â†’ search â†’ context)
3. Response generation (context â†’ LLM â†’ answer)

Usage:
    python test_rag_pipeline.py [--full-ingestion]

Options:
    --full-ingestion    Run full ingestion before testing (scrapes real sites)
"""

import sys
import argparse
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from src.rag.chunker import DocumentChunker, ChunkingStrategy
from src.rag.retriever import RAGRetriever
from src.vectorstore.qdrant_client import QdrantVectorStore
from src.utils.logger import setup_logger, get_logger

# Initialize logger
setup_logger()
logger = get_logger()


def print_section(title):
    """Print a section header."""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


def test_chunking():
    """Test document chunking."""
    print_section("TEST 1: Document Chunking")

    chunker = DocumentChunker(chunk_size=300, chunk_overlap=50)

    # Test documents (Ukrainian content)
    test_docs = [
        {
            "text": """
            ÐŸÑ€Ð¾Ð³Ñ€Ð°Ð¼Ð° "Ð”Ð¾Ð´Ð¾Ð¼Ñƒ Ð´Ð»Ñ Ð£ÐºÑ€Ð°Ñ—Ð½Ð¸" (Homes for Ukraine) Ð±ÑƒÐ»Ð° Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð° Ñƒ 2022 Ñ€Ð¾Ñ†Ñ–.
            Ð¦Ñ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð° Ð´Ð¾Ð·Ð²Ð¾Ð»ÑÑ” ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¸Ð¼ Ð±Ñ–Ð¶ÐµÐ½Ñ†ÑÐ¼ Ð·Ð½Ð°Ð¹Ñ‚Ð¸ Ð¶Ð¸Ñ‚Ð»Ð¾ Ñƒ Ð’ÐµÐ»Ð¸ÐºÐ¾Ð±Ñ€Ð¸Ñ‚Ð°Ð½Ñ–Ñ—.
            Ð’Ñ–Ð·Ð¸ Ð²Ð¸Ð´Ð°ÑŽÑ‚ÑŒÑÑ Ñ‚ÐµÑ€Ð¼Ñ–Ð½Ð¾Ð¼ Ð´Ð¾ Ñ‚Ñ€ÑŒÐ¾Ñ… Ñ€Ð¾ÐºÑ–Ð². ÐŸÑ–ÑÐ»Ñ Ð·Ð°ÐºÑ–Ð½Ñ‡ÐµÐ½Ð½Ñ Ð²Ñ–Ð·Ð¸ Ð¼Ð¾Ð¶Ð½Ð° Ð¿Ð¾Ð´Ð°Ñ‚Ð¸
            Ð·Ð°ÑÐ²Ñƒ Ð½Ð° Ð¿Ñ€Ð¾Ð´Ð¾Ð²Ð¶ÐµÐ½Ð½Ñ. Ð’Ð°Ð¶Ð»Ð¸Ð²Ð¾ Ð·Ð°Ñ€ÐµÑ”ÑÑ‚Ñ€ÑƒÐ²Ð°Ñ‚Ð¸ÑÑ Ñƒ Ð¼Ñ–ÑÑ†ÐµÐ²Ñ–Ð¹ Ð¿Ð¾Ð»Ñ–ÐºÐ»Ñ–Ð½Ñ–Ñ†Ñ– Ð¿Ñ€Ð¾Ñ‚ÑÐ³Ð¾Ð¼
            Ð¿ÐµÑ€ÑˆÐ¸Ñ… ÐºÑ–Ð»ÑŒÐºÐ¾Ñ… Ñ‚Ð¸Ð¶Ð½Ñ–Ð² Ð¿Ñ–ÑÐ»Ñ Ð¿Ñ€Ð¸Ð±ÑƒÑ‚Ñ‚Ñ Ð´Ð¾ Ð’ÐµÐ»Ð¸ÐºÐ¾Ð±Ñ€Ð¸Ñ‚Ð°Ð½Ñ–Ñ—.
            """,
            "metadata": {"source": "test_govuk", "topic": "visa", "language": "uk"}
        },
        {
            "text": """
            Ð£ÐºÑ€Ð°Ñ—Ð½Ñ†Ñ– Ñƒ Ð’ÐµÐ»Ð¸ÐºÐ¾Ð±Ñ€Ð¸Ñ‚Ð°Ð½Ñ–Ñ— Ð¼Ð°ÑŽÑ‚ÑŒ Ð¿Ñ€Ð°Ð²Ð¾ Ð½Ð° Ñ€Ð¾Ð±Ð¾Ñ‚Ñƒ. Ð”Ð»Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¸ Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ð¾
            Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ñ‚Ð¸ Ð½Ð¾Ð¼ÐµÑ€ National Insurance (NI). Ð¦ÐµÐ¹ Ð½Ð¾Ð¼ÐµÑ€ Ð¼Ð¾Ð¶Ð½Ð° Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ñ‚Ð¸ Ð¾Ð½Ð»Ð°Ð¹Ð½
            Ð°Ð±Ð¾ Ð·Ð° Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½Ð¾Ð¼. Ð‘ÐµÐ· NI Ð½Ð¾Ð¼ÐµÑ€Ð° Ð²Ð¸ Ð½Ðµ Ð·Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð¿Ñ€Ð°Ñ†ÑŽÐ²Ð°Ñ‚Ð¸ Ð¾Ñ„Ñ–Ñ†Ñ–Ð¹Ð½Ð¾.
            Ð¢Ð°ÐºÐ¾Ð¶ Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ð¾ Ð·Ð°Ñ€ÐµÑ”ÑÑ‚Ñ€ÑƒÐ²Ð°Ñ‚Ð¸ÑÑ Ñƒ Ð¿Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ñ–Ð¹ ÑÐ»ÑƒÐ¶Ð±Ñ– HMRC.
            """,
            "metadata": {"source": "test_opora", "topic": "work", "language": "uk"}
        }
    ]

    chunks = chunker.chunk_documents(test_docs)

    print(f"âœ“ Created {len(chunks)} chunks from {len(test_docs)} documents")

    for i, chunk in enumerate(chunks, 1):
        print(f"\nChunk {i}:")
        print(f"  Source: {chunk.metadata.get('source')}")
        print(f"  Topic: {chunk.metadata.get('topic')}")
        print(f"  Length: {len(chunk.text)} chars")
        print(f"  Text: {chunk.text[:100].strip()}...")

    return chunks


def test_vector_store(chunks):
    """Test vector store operations."""
    print_section("TEST 2: Vector Store Operations")

    vector_store = QdrantVectorStore()

    # Connect
    print("Connecting to Qdrant...")
    if not vector_store.connect():
        print("âœ— Failed to connect to Qdrant")
        return False

    print("âœ“ Connected to Qdrant")

    # Check for embedding capability
    print("\nTesting embedding generation...")
    test_embedding = vector_store.get_embedding("Ñ‚ÐµÑÑ‚")

    if not test_embedding:
        print("âœ— Failed to generate embedding (Ollama not available)")
        print("  This test requires Ollama to be running with mxbai-embed-large")
        return False

    print(f"âœ“ Generated test embedding ({len(test_embedding)} dimensions)")

    # Create collection
    print("\nCreating/verifying collection...")
    vector_size = len(test_embedding)

    if not vector_store.create_collection(vector_size=vector_size):
        print("Collection already exists or creation not needed")
    else:
        print("âœ“ Collection created")

    # Get collection info
    info = vector_store.get_collection_info()
    if info:
        print(f"âœ“ Collection: {info['name']}")
        print(f"  Points: {info['points_count']}")
        print(f"  Vector size: {info['vector_size']}")

    # Store chunks
    print(f"\nStoring {len(chunks)} test chunks...")
    docs_to_store = [chunk.to_dict() for chunk in chunks]

    if vector_store.add_documents(docs_to_store):
        print(f"âœ“ Stored {len(chunks)} chunks successfully")

        # Verify storage
        info = vector_store.get_collection_info()
        if info:
            print(f"âœ“ Collection now has {info['points_count']} total points")

        return True
    else:
        print("âœ— Failed to store chunks")
        return False


def test_retrieval():
    """Test document retrieval."""
    print_section("TEST 3: Document Retrieval")

    retriever = RAGRetriever()

    # Initialize
    print("Initializing retriever...")
    if not retriever.initialize():
        print("âœ— Failed to initialize retriever")
        return False

    print("âœ“ Retriever initialized")

    # Test queries (Ukrainian)
    test_queries = [
        "Ð¯Ðº Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ñ‚Ð¸ Ð²Ñ–Ð·Ñƒ Ð´Ð¾ Ð’ÐµÐ»Ð¸ÐºÐ¾Ð±Ñ€Ð¸Ñ‚Ð°Ð½Ñ–Ñ—?",
        "Ð©Ð¾ Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ð¾ Ð´Ð»Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¸ Ð² UK?",
        "Ð¯Ðº Ð·Ð°Ñ€ÐµÑ”ÑÑ‚Ñ€ÑƒÐ²Ð°Ñ‚Ð¸ÑÑ Ñƒ Ð»Ñ–ÐºÐ°Ñ€Ñ?",
    ]

    for query in test_queries:
        print(f"\nðŸ“ Query: {query}")
        print("-" * 70)

        result = retriever.retrieve(query, top_k=3)

        if result.found_documents > 0:
            print(f"âœ“ Found {result.found_documents} relevant documents")
            print(f"  Context length: {len(result.context)} characters")

            print("\nSources:")
            for i, source in enumerate(result.sources, 1):
                metadata = source.get('metadata', {})
                print(f"  {i}. Topic: {metadata.get('topic', 'unknown')}")
                print(f"     Source: {metadata.get('source', 'unknown')}")
                print(f"     Score: {source['score']:.3f}")
                print(f"     Text: {source['text'][:80]}...")
        else:
            print("âš  No documents found")

    # Health check
    print("\nRetriever health check:")
    health = retriever.health_check()
    print(f"  Status: {health['status']}")
    print(f"  Healthy: {health['healthy']}")
    if health['healthy']:
        print(f"  Documents: {health.get('documents', 0)}")

    return True


def test_full_pipeline():
    """Test the complete RAG pipeline end-to-end."""
    print_section("COMPLETE RAG PIPELINE TEST")

    success_count = 0
    total_tests = 3

    try:
        # Test 1: Chunking
        chunks = test_chunking()
        if chunks and len(chunks) > 0:
            success_count += 1
        else:
            print("\nâœ— Chunking test failed")
            return False

        # Test 2: Vector Store
        if test_vector_store(chunks):
            success_count += 1
        else:
            print("\nâœ— Vector store test failed")
            print("  Make sure Ollama is running and accessible")
            return False

        # Test 3: Retrieval
        if test_retrieval():
            success_count += 1
        else:
            print("\nâœ— Retrieval test failed")
            return False

        # Summary
        print_section("TEST SUMMARY")
        print(f"\nTests passed: {success_count}/{total_tests}")

        if success_count == total_tests:
            print("\nâœ“ ALL TESTS PASSED!")
            print("\nThe RAG pipeline is working correctly:")
            print("  âœ“ Documents can be chunked")
            print("  âœ“ Embeddings can be generated")
            print("  âœ“ Chunks can be stored in Qdrant")
            print("  âœ“ Relevant documents can be retrieved")
            print("\nNext steps:")
            print("  1. Run full ingestion: python run_ingestion.py --recreate")
            print("  2. Test with real queries via the bot")
            return True
        else:
            print(f"\nâœ— {total_tests - success_count} test(s) failed")
            return False

    except Exception as e:
        logger.exception(f"Pipeline test failed: {e}")
        print(f"\nâœ— Fatal error: {e}")
        return False


def run_full_ingestion():
    """Run full ingestion before testing."""
    print_section("RUNNING FULL INGESTION")

    try:
        from src.rag.ingestion import run_ingestion

        print("\nThis will scrape real websites and populate the database.")
        print("This may take several minutes...\n")

        stats = run_ingestion(
            scrape_govuk=True,
            scrape_opora=True,
            recreate_collection=True,
            save_artifacts=True
        )

        print(f"\nIngestion completed:")
        print(f"  Success: {stats.success}")
        print(f"  Documents: {stats.documents_scraped}")
        print(f"  Chunks: {stats.chunks_stored}")
        print(f"  Duration: {stats.duration_seconds:.1f}s")

        return stats.success

    except Exception as e:
        logger.exception(f"Ingestion failed: {e}")
        print(f"\nâœ— Ingestion error: {e}")
        return False


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Test the complete RAG pipeline end-to-end"
    )
    parser.add_argument(
        '--full-ingestion',
        action='store_true',
        help='Run full ingestion before testing (scrapes real websites)'
    )

    args = parser.parse_args()

    print("\n" + "=" * 70)
    print("  RAG PIPELINE END-TO-END TEST")
    print("=" * 70)

    # Run full ingestion if requested
    if args.full_ingestion:
        if not run_full_ingestion():
            print("\nâœ— Ingestion failed, cannot continue with tests")
            return 1

    # Run tests
    success = test_full_pipeline()

    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())