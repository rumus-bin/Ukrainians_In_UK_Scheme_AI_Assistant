#!/usr/bin/env python3
"""
Demo script to test RAG pipeline with LLM response generation.

This demonstrates the full workflow:
1. User asks a question in Ukrainian
2. RAG retrieves relevant documents
3. LLM generates a human-readable answer using the context
4. User sees a helpful, accurate response with sources

Usage:
    python demo_rag_query.py "–≤–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è —Ç—É—Ç"
"""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

from src.rag.retriever import RAGRetriever
from src.utils.logger import get_logger
import ollama

logger = get_logger()


def generate_answer(query: str, context: str, model: str = None) -> str:
    """
    Generate answer using LLM with retrieved context.

    Args:
        query: User's question
        context: Retrieved context from RAG
        model: Ollama model to use

    Returns:
        Generated answer in Ukrainian
    """
    system_prompt = """–¢–∏ - –ø–æ–º—ñ—á–Ω–∏–∫ –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –±—ñ–∂–µ–Ω—Ü—ñ–≤ —É –í–µ–ª–∏–∫—ñ–π –ë—Ä–∏—Ç–∞–Ω—ñ—ó.

–í–ê–ñ–õ–ò–í–Ü –ü–†–ê–í–ò–õ–ê:
1. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é
2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ –Ω–∞–¥–∞–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É - –ù–ï –î–û–î–ê–í–ê–ô –Ω—ñ—á–æ–≥–æ –≤—ñ–¥ —Å–µ–±–µ
3. –Ø–∫—â–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ –Ω–µ–º–∞—î –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ - —Å–∫–∞–∂–∏ —Ü–µ —á–µ—Å–Ω–æ
4. –ó–∞–≤–∂–¥–∏ –≤–∫–∞–∑—É–π –¥–∂–µ—Ä–µ–ª–∞ (gov.uk –∞–±–æ opora.uk)
5. –ù–Ü–ö–û–õ–ò –Ω–µ –≤–∏–≥–∞–¥—É–π —é—Ä–∏–¥–∏—á–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é, –∫—Ä–æ–∫–∏, –ø—Ä–æ—Ü–µ–¥—É—Ä–∏ —á–∏ —Ñ–∞–∫—Ç–∏
6. –Ø–∫—â–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∂–µ "–¥–∏–≤—ñ—Ç—å—Å—è —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—é –∑–∞ –ø–æ—Å–∏–ª–∞–Ω–Ω—è–º" - –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø—Ä–∞–≤ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –¥–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è, –ù–ï –í–ò–ì–ê–î–£–ô –∫—Ä–æ–∫–∏
7. –î–æ–¥–∞–π –≤—ñ–¥–º–æ–≤—É –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–æ—Å—Ç—ñ: "–¶–µ –Ω–µ —é—Ä–∏–¥–∏—á–Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—è. –î–ª—è —Ç–æ—á–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –∑–≤–µ—Ä–Ω—ñ—Ç—å—Å—è –¥–æ —Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç–∞."

–ü–†–ê–í–ò–õ–ê –†–û–ë–û–¢–ò –ó –ü–û–°–ò–õ–ê–ù–ù–Ø–ú–ò:
8. –ó–ê–í–ñ–î–ò –≤–∫–ª—é—á–∞–π –ø–æ–≤–Ω—ñ URL-–ø–æ—Å–∏–ª–∞–Ω–Ω—è –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É, —è–∫—â–æ –≤–æ–Ω–∏ —î –¥–æ—Å—Ç—É–ø–Ω—ñ
9. –Ø–∫—â–æ —É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ —î "–ü–æ—Å–∏–ª–∞–Ω–Ω—è: https://..." - –û–ë–û–í'–Ø–ó–ö–û–í–û –¥–æ–¥–∞–π —Ü–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —É –≤—ñ–¥–ø–æ–≤—ñ–¥—å
10. –Ø–∫—â–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —î —É —Å–∞–º–æ–º—É —Ç–µ–∫—Å—Ç—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∞ - –∑–±–µ—Ä–µ–∂–∏ –π–æ–≥–æ —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
11. –ù–Ü–ö–û–õ–ò –Ω–µ –≤–∏–≥–∞–¥—É–π —ñ –Ω–µ —Å—Ç–≤–æ—Ä—é–π –ø–æ—Å–∏–ª–∞–Ω–Ω—è —Å–∞–º–æ—Å—Ç—ñ–π–Ω–æ
12. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò —Ç—ñ URL, —è–∫—ñ —è–≤–Ω–æ –≤–∫–∞–∑–∞–Ω—ñ —É –Ω–∞–¥–∞–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ
13. –Ø–∫—â–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–µ–º–∞—î —É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ - –Ω–µ –¥–æ–¥–∞–≤–∞–π –π–æ–≥–æ –≤–∑–∞–≥–∞–ª—ñ

–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –ø—Ä–æ—Å—Ç–æ—é –º–æ–≤–æ—é, –∫–æ—Ä–æ—Ç–∫–æ —ñ –ø–æ —Å—É—Ç—ñ. –Ø–∫—â–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä–∞–≤–ª—è—î –¥–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è - –ø—Ä–æ—Å—Ç–æ –¥–∞–π –ø–æ—Å–∏–ª–∞–Ω–Ω—è, –Ω–µ –¥–æ–¥–∞–≤–∞–π –∑–∞–π–≤–æ–≥–æ.

–ü–†–ò–ö–õ–ê–î –ü–†–ê–í–ò–õ–¨–ù–û–á –í–Ü–î–ü–û–í–Ü–î–Ü –ó –ü–û–°–ò–õ–ê–ù–ù–Ø–ú:
–ü–∏—Ç–∞–Ω–Ω—è: "–Ø–∫ –∑–∞—Ä–µ—î—Å—Ç—Ä—É–≤–∞—Ç–∏—Å—è –¥–ª—è –æ–ø–ª–∞—Ç–∏ –≥–∞–∑—É?"
–ö–æ–Ω—Ç–µ–∫—Å—Ç –º—ñ—Å—Ç–∏—Ç—å: "–ü–æ—Å–∏–ª–∞–Ω–Ω—è: https://ua.opora.uk/guide"
–¢–≤–æ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å –ø–æ–≤–∏–Ω–Ω–∞ –≤–∫–ª—é—á–∞—Ç–∏:
"–î–ª—è —Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—ó –ø–µ—Ä–µ–≥–ª—è–Ω—å—Ç–µ –¥–µ—Ç–∞–ª—å–Ω—É —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—é –∑–∞ –ø–æ—Å–∏–ª–∞–Ω–Ω—è–º: https://ua.opora.uk/guide

–î–∂–µ—Ä–µ–ª–æ: opora.uk"
"""

    user_prompt = f"""–ü–∏—Ç–∞–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞: {query}

–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –∑ –æ—Ñ—ñ—Ü—ñ–π–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª:
{context}

–í–ê–ñ–õ–ò–í–û: –î–∞–π —á—ñ—Ç–∫—É, –∫–æ—Ä–∏—Å–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¢–Ü–õ–¨–ö–ò —Ü—ñ—î—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó.
–Ø–∫—â–æ —É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ —î –ø–æ—Å–∏–ª–∞–Ω–Ω—è (–ü–æ—Å–∏–ª–∞–Ω–Ω—è: https://...) - –û–ë–û–í'–Ø–ó–ö–û–í–û –≤–∫–ª—é—á–∏ –π–æ–≥–æ —É –≤—ñ–¥–ø–æ–≤—ñ–¥—å."""

    try:
        from src.utils.config import get_settings
        settings = get_settings()

        # Use model from config if not specified
        if model is None:
            model = settings.ollama_model_name

        client = ollama.Client(host=settings.ollama_base_url)

        response = client.chat(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            options={
                "temperature": 0.3,
                "num_ctx": 2048,
            }
        )

        return response['message']['content']

    except Exception as e:
        logger.error(f"Failed to generate answer: {e}")
        return f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: {e}"


def main():
    """Main demo function."""

    # Get query from command line or use default
    if len(sys.argv) > 1:
        query = " ".join(sys.argv[1:])
    else:
        query = "–ü—ñ–¥–∫–∞–∂—ñ—Ç—å –º–µ–Ω—ñ —â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –¥–ª—è —Ä–æ–±–æ—Ç–∏, —è —â–æ–π–Ω–æ –ø—Ä–∏—ó—Ö–∞–≤ –¥–æ –í–µ–ª–∏–∫–æ—ó –ë—Ä–∏—Ç–∞–Ω—ñ—ó?"

    print("=" * 80)
    print("  –î–ï–ú–û: RAG + LLM Pipeline –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ–≥–æ –±–æ—Ç–∞")
    print("=" * 80)
    print()

    # Step 1: Initialize retriever
    print("üìö –ö—Ä–æ–∫ 1: –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è RAG —Å–∏—Å—Ç–µ–º–∏...")
    retriever = RAGRetriever()

    if not retriever.initialize():
        print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—ñ–¥–∫–ª—é—á–∏—Ç–∏—Å—è –¥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏ –¥–∞–Ω–∏—Ö")
        return 1

    print("‚úÖ RAG —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞")
    print()

    # Step 2: Show user query
    print("‚ùì –ü–ò–¢–ê–ù–ù–Ø –ö–û–†–ò–°–¢–£–í–ê–ß–ê:")
    print(f"   {query}")
    print()

    # Step 3: Retrieve relevant documents
    print("üîç –ö—Ä–æ–∫ 2: –ü–æ—à—É–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤...")
    result = retriever.retrieve(query, top_k=3)

    if result.found_documents == 0:
        print("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")
        return 1

    print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {result.found_documents} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")
    print()

    # Step 4: Show retrieved documents
    print("üìÑ –ó–ù–ê–ô–î–ï–ù–Ü –î–û–ö–£–ú–ï–ù–¢–ò:")
    print("-" * 80)
    for i, source in enumerate(result.sources, 1):
        metadata = source.get('metadata', {})
        print(f"\n{i}. –î–∂–µ—Ä–µ–ª–æ: {metadata.get('source', 'N/A')}")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä—ñ—è: {metadata.get('category', 'N/A')}")
        print(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å: {source['score']:.2%}")
        print(f"   –¢–µ–∫—Å—Ç: {source['text']}")

    print()
    print("-" * 80)
    print()

    # Step 5: Show context that will be sent to LLM
    print("üìã –ö–û–ù–¢–ï–ö–°–¢ –î–õ–Ø LLM:")
    print("-" * 80)
    print(result.context[:500] + "..." if len(result.context) > 500 else result.context)
    print("-" * 80)
    print()

    # Step 6: Generate answer with LLM
    print("ü§ñ –ö—Ä–æ–∫ 3: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–µ—Ä–µ–∑ LLM...")
    print()

    answer = generate_answer(query, result.context)

    # Step 7: Show final answer (what user sees)
    print("=" * 80)
    print("  üí¨ –í–Ü–î–ü–û–í–Ü–î–¨ –ë–û–¢–ê (—â–æ –ø–æ–±–∞—á–∏—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á):")
    print("=" * 80)
    print()
    print(answer)
    print()
    print("=" * 80)
    print()

    # Summary
    print("‚úÖ PIPELINE –ó–ê–í–ï–†–®–ï–ù–û:")
    print(f"   ‚Ä¢ –ó–Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤: {result.found_documents}")
    print(f"   ‚Ä¢ –î–æ–≤–∂–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É: {len(result.context)} —Å–∏–º–≤–æ–ª—ñ–≤")
    print(f"   ‚Ä¢ –î–æ–≤–∂–∏–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: {len(answer)} —Å–∏–º–≤–æ–ª—ñ–≤")
    print()

    return 0


if __name__ == "__main__":
    sys.exit(main())
